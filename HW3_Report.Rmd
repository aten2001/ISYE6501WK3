---
title: "ISYE6501 HW3"
author: "Keh-Harng Feng"
date: "May 29, 2017"
output: 
  bookdown::html_document2:
    fig_caption: TRUE
    toc: FALSE
urlcolor: blue
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, tidy = TRUE, cache = TRUE)
options(digits = 4)

library('caret')
library('corrplot')
library('reshape2')
library('gtools')
```

## Preface
This is a reproducible report with most of the codes doing the heavy lifting hidden in the background. **All codes are available to be audited.** If you wish to check the various scripts and code snippets used for the computations, you can download the source code of the report by [clicking here](https://github.com/fengkehh/ISYE6501WK3/blob/master/HW3_Report.Rmd). However, as a general rule of thumb you should NOT run any downloaded R scripts from an untrusted source on your computer without understanding the source code first.

# Question 1
**Describe a situation or problem from your job, everyday life, current events, etc., for which exponential smoothing would be appropriate. What data would you need? Would you expect the value of $\alpha$ (the first smoothing parameter) to be closer to 0 or 1, and why?**

I worked on a computer model that simulates the transfer of radiation through the atmosphere. Clearly the amount of radiation that reaches the surface will depend on things such as the incident angle and the amount of radiation that reaches the top of the atmosphere. The current incident angle can be computed based on the orbital characteristics between the Earth and the Sun. However, the exact amount of energy outputted by the sun is highly dependent on solar activities and is not a simple task to model using physics. If data is lacking for a particular period of interest we can use exponential smoothing to produce an estimation. 

Data required would be the measured top of the atmosphere solar irradiance in $W/m^2$ set at a reference distance at a fixed time step size. Note that the Earth's orbit is not circular and thus the distance between the Earth and the Sun will change constantly. Nevertheless it is very easy to work backwards to compute the irradiance at a reference distance by knowing the exact spot Earth and the measuring satellite are at currently and assuming isotropic radiation from the Sun. 

Judging by [this chart](https://smd-prod.s3.amazonaws.com/science-red/s3fs-public/mnt/medialibrary/2013/01/08/tsi_composite_strip.jpg) there is some form of seasonality on the time scale of decades. If data is measured monthly it would be best to set $\alpha$ to a  small number since the time scale that is really of interest is much larger than the time step.

# Question 2
**Using the 20 years of daily high temperature data for Atlanta (July through October) from Homework 2 Question 5, build and use an exponential smoothing model to help make a judgment of whether the unofficial end of summer has gotten later over the 20 years. (Part of the point of this assignment is for you to think about how you might use exponential smoothing to answer this question.) Note: in R, you can use either HoltWinters (simpler to use) or the smooth package’s es function (harder to use, but more general). If you use es, the Holt-Winters model uses model=”AAM” in the function call (the first and second constants are used “A”dditively, and the third (seasonality) is used “M”ultiplicatively; the documentation doesn’t make that clear).**



# Question 3
**Describe a situation or problem from your job, everyday life, current events, etc., for which a linear regression model would be appropriate. List some (up to 5) predictors that you might use.**

Recently I completed a project that involves analyzing the effects of different factors on automobile fuel efficiency. The data includes measured fuel effciency of different vehicles in the forms of miles per gallon and various physical characteristics of the cars. The main question was if the types of transmission (auto or manual) had an effect on fuel effciency. My strategy to answer that question was to build a multivariate linear regression model to fit MPG on pertinent predictors and see if transmission type was a significant predictor in the model. The following predictors were chosen after feature selection:

`Transmission Type`

`Number of Engine Cylinders`

`Weight`

`Horsepower`

Hypothesis test carried out on model coefficients showed that transmission is NOT an important factor when it comes to fuel efficiency. The full report can be accessed [here](https://fengkehh.github.io/post/2017-03-30-can-car-transmission-affect-fuel-efficiency/).

# Question 4

**Using crime data from http://www.statsci.org/data/general/uscrime.txt (description at http://www.statsci.org/data/general/uscrime.html ), use regression (a useful R function is lm or glm) to predict the observed crime rate in a city with [the data shown on handout].**

**Show your model (factors used and their coefficients), the software output, and the quality of fit.**

## Data Preprocess & Exploratory Analysis

TODO: Strategy -- show there are no near zero variance predictors. Show pair matrix plot. Show strong correlation between P01 and P02 using corrPlot. Use findCorrelation to identify P01 as the predictor to be removed (reference Kuhn for algorithm). 

Scale & center.

The data is loaded with header. The first 5 rows are shown below:

```{r}
if (!file.exists('uscrime.txt')) {
    download.file('http://www.statsci.org/data/general/uscrime.txt', destfile = 'uscrime.txt')
}

q5_data <- read.table('uscrime.txt', header = TRUE)

head(q5_data)

q5_data$So <- as.factor(q5_data$So)

data.predictors <- q5_data[,!(names(q5_data) %in% c('Crime'))]
```

The 16th column `Crime` is the response. The second column `So` is a categorical variable with two levels `0` and `1`. A quick check shows that no predictors exhibit near zero variance:

```{r, echo = TRUE}
nzv <- nearZeroVar(data.predictors, saveMetrics = TRUE)
nzv
```

Figure \@ref(fig:pairsplot) shows the Pair-wise scatter plots between all predictors. There seems to be colinearity between `Po1` and `Po2`. Some other predictor pairs are also suspect such as `Ed` and `Wealth`, `Wealth` and `Ineq`, `Po1` and `Wealth`, and finally `Po2` and `Wealth`. A plot of the correlations between all *non-categorical* predictors is shown in Figure \@ref(fig:corrplot).

```{r pairsplot, fig.cap = 'Pair scatterplot matrix of all the predictors. Matrix is symmetric by design.'}

pairs(data.predictors)

```


```{r corrplot, fig.cap = 'Cluster plot of the correlation matrix. Matrix is symmetric by design.'}
data.noncat <- data.predictors[,!(names(data.predictors) %in% c('So'))]
corrmat <- correlations <- cor(data.noncat)

corrplot(corrmat, order = 'hclust')
```

As suspected, Figure \@ref(fig:corrplot) shows strong correlations between (`Po1`, `Po2`), (`Ineq`, `Wealth`) and (`Ineq`, `Ed`). The following algorithm described by Kuhn and Johnson in [Applied Predictive Modeling](appliedpredictivemodeling.com) in the form of the `findCorrelations()` function from the `caret` package is used to identify highly colinear predictors to be removed:

> 1. Calculate the correlation matrix.
> 2. Determine the two predictors associated with the largest absolute parwise correlations, (A, B).
> 3. Determine the average correlation between A and other predictors. Do the same for B.
> 4. Remove the predictor with larger average correlation.
> 5. Repeat 2 - 4 until no correlations are above the threshold.

For this step the threshold value is set to the default value, 0.9. The algorithm found the following predictor(s) to be removed due to colinearity:

```{r}
tbr <- findCorrelation(corrmat, names = TRUE)
tbr
```

Since the question does not require us to interpret the model based on model coefficients, data transformation, scaling and centering are viable options. A list of density plots are shown in Figure \@ref(fig:untransformed)

```{r untransformed, fig.cap = 'Density plots of untransformed predictors.'}
melted.noncat <- melt(data.noncat[,!(names(data.noncat) %in% c('So', tbr))])

densityplot(~value|variable, data = melted.noncat, scales = list(x = list(relation = 'free'), y = list(relation = 'free')), adjust = 1.25, pch = '|', xlab = 'Predictor')
```

Some of the predictors exihibit right skew, in particular `Prob`, `Pop`, `NW`, `U1`, `U2`, `M`, `Po2` and `M.F`. `Wealth` exhibits left skew. The predictors are transformed using Box-Cox transformation then centered and scaled in an attempt to fix this. The result of the transformation is shown in Figure \@ref(fig:transformed).

```{r transformed, fig.cap = 'Transformed predictor density plots. Transformations applied: center, scale and Yeo-Johnson (plus removal of highly colinear predictor).}
#data.dummy <- as.data.frame(cbind(model.matrix(Crime ~ ., data = q5_data), Crime = q5_data$Crime))

#data.dummy <- as.data.frame(model.matrix(Crime ~ ., data = q5_data))

preproc <- preProcess(data.noncat, method = c('center', 'scale', 'BoxCox', 'corr'))

data.processed <- predict(preproc, data.noncat)

melted.processed <- melt(data.processed)
densityplot(~value|variable, data = melted.processed, scales = list(x = list(relation = 'free'), y = list(relation = 'free')), adjust = 1.25, pch = '|', xlab = 'Predictor')
```

With the exception of `M.F` the improvement to the predictor distribution is quite remarkable. This set of transformations shall be used prior to automatic feature selection with the addition of the categorical predictor `So`.

## Algorithmic Feature Selection
```{r}
var_combo <- permutations(2, 14, v = c('TRUE', 'FALSE'), repeats.allowed = TRUE)[-1,]
```

The preprocessed data is now split into a test & training set. With 14 predictors (13 preprocessed numerical predictors + `So`), the number of predictor combinations is `r nrow(var_combo) - 1` (the trivial combination where no predictor is selected is not included). An exhaustive search over the entire combination space is carried out. 10-fold cross-validation on the training data with the average of MSE from the model prediction on the validation fold is used to estimate out-of-sample accuracy. The predictor combination that results in the the lowest out-of-sample accuracy estimate is selected.

## Performance Evaluation

Retrain using full training set. Compute prediction on test set & show R^2 or MSE.

## Prediction

Make prediction using specified predictor values.
